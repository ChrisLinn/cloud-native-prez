+ 单体架构
    * a 出问题，b 跟着遭殃
        - 硬件隔离
            + 存储
                * 手动 cgroups 限定资源
                * 物理隔离就是加机器 加机器
                    - 成本
                    - 单应用并发能力不成线性关系
                        + 服务拆分
    * MySQL 分表
        - 加机器
    * 抢红包
        - 微服务
            + 加机器
        - 熔断降级速率限制
            + 遥测
        - 弹性云

---
+ 微服务
    * _服务发现_
        - traditional
            + client-side
                * code/framework in app
                    - Eureka
        - k8s
            + network layer
                * dns
                * service
                * ingress
    * 单机、主备、集群、异地容灾
    * 扩容
    * load-balancing
        - traditional
            + ribbon
    - 流量监控
        - 限流, 降级
            + 在2014年春节的时候，微信红包，每分钟8亿多次的请求，其实真正到它后台的请求量，只有十万左右的数量级（这里的数据可能不准），剩余的流量在接口层就被挡住了；
            * 遥测
        - 熔断
            * 遥测
    - 错误异常监控
        + 重试
        - 故障恢复
            * 遥测
            + 平均故障时间 vs 故障处理时间
        - 日志收集
            + 面对大规模服务和节点时，进入服务器查看日志文件是不现实的
        - k8s
    * 架构转换 : 单体架构  -> 微服务
        - traffic management
        - 线上业务不能停
        - 消息中间件
            + 可以把两个模块之间的交互异步化，其次可以把不均匀请求流量输出为匀速的输出流量，所以说消息中间件 异步化 解耦 和流量削峰的利器。

---
+ 加机器
    * 运行时、依赖库
        - cpu 架构、os、编译器
            + int
    * 负载均衡
        - 流量入口, 高性能、高可靠
            + dns
                * 延迟
                    - DNS 解析是多级解析，新增/修改 DNS 后，解析时间较长；解析过程中，用户访问网站将失败；
                * 策略 simple, inflexible
                * 控制权限小
                    - 扩展性低：DNS 负载均衡的控制权在域名商那里，无法对其做更多的改善和扩展；
                * 维护性差：也不能反映服务器的当前运行状态；支持的算法少；不能区分服务器的差异（不能根据系统与服务的状态来判断负载）
            + 硬件
                * 更适用于 __一大堆设备__ 、大访问量、 __简单应用__
                    - 如果几台服务器，用F5之类的硬件产品显得有些浪费
                * 只能流量，无法内存
                    - 无法有效掌握服务器及应用状态
                        + 根据系统与应用的状况来分配负载
                    - 硬件负载均衡，一般都不管实际系统与应用的状态，而只是从网络层来判断，所以有时候系统处理能力已经不行了，但网络可能还来得及反应(这种情况比较典型，比如应用服务器后面内存已经占用很多，但还没有彻底不行，如果网络传输量不大就未必在网络层能反映出来)
                * 成本
                    - 硬件成本：中低端硬件负载均衡价格在数十万，高端的上百万，价格非常昂贵。当我们需要组成一个高可用集群时，需要数台机器，成本异常高。
                        + F5: 15w ~ 55w
                        + A10: 55w ~ 100w
                    - 人力成本：硬件负载均衡功能比较强大，配置比较灵活，这也导致在维护上，我们需要一些经过专业培训的人员，就增加了人力成本。
                    - 时间成本：当使用的过程中遇到bug或者新需求需要厂商提供新版本的时候，我们需要经过繁琐的流程向厂商上报，然后厂商再发布新版本供我们升级，时间周期非常长，在高速发展的互联网行业，这种周期是无法接受的。
                * single point
                * k8s 动态扩容
                    - 一个命令动态扩展10个 nginx 节点，线上运行
                    - 流量洪峰, 响应调度
                    - 一般软件负载均衡支持到 5 万级并发已经很困难了，硬件负载均衡可以支持；
                        + 但成本
    * 折旧率
        - 想放上云
            + 弹性计算
                * _无状态_
                    - 声明式 vs 命令式
                    - 幂等
                    - 横向扩展, 水平伸缩
                        + 反向代理
                    - 有状态也可能涉及锁，导致并发的串行化

---
+ 虚拟机
    * 隔离
    * 依赖库
    * 吃资源启动慢, 宕机时间, 云产商成本
        - 容器, 进程角度
            + docker
                * namespaces
                    - ipc, posix
                    - pid
                    - network
                        + device
                        + protocol
                        + firewall
                        + routing
                        + host, domain
                        + user group
                * cgroups
                    - mem
                    - cpu
                    - devices
                    - hang process
                    - bandwidth, flow_priv
            + 应用才是价值所在
                * compose
                * swarm
                * k8s

---
+ k8s
    * 应用的生命周期管理
        - 部署和管理
            + 扩缩容
            + 自动恢复
            + 发布
    * 为微服务提供了可扩展、高弹性的部署和管理平台

---
+ 接口，解耦产品
+ pod
    * 部署应用服务的最小单元
    * 提供容器运行时环境并保持容器的运行状态
    * 共享网络文件
    * example
        - 发布软件，从源同步
        - 文件服务，拉取文件
        - 返回币价，拉取币价
        - log saving
    * 各自构建自己的容器镜像, 独立版本管理、编译、发布、更新
        - sidecar
            + 剥离所有的安全、服务发现、负载均衡等问题，让服务开发人员只用关注业务逻辑，可以完美支持多语言
    * 组合成一个微服务对外提供服务
+ 服务
    * 暴露
    * pod 只是一个运行服务的实例(process)，随时可能在一个节点上停止，在另一个节点上以一个新的ip启动一个新的pod，因此不能以确定的ip 和端口号提供服务
        - pod 临时（非持久）
+ b 站 app被逆向，app key 泄漏，内网api
    + auth
    * 内网 ip
+ 故障恢复
    * Kubernetes具备超强的**故障恢复机制**，Kubernetes会重启意外停止的pod。我们曾遇到过因内存泄露，导致容器在一天内宕机多次的情况，然而令人惊讶的是，甚至我们自己都没有察觉到。
        - 监控: 一旦问题出现，即使被及时处理了，还是想要知道

---
+ k8s 成本
    * etcd 
        - 重要角色, 集群可用性上限
            + 服务配置发现信息&配置信息
        - 3, 5, 7
    * docker 成本
        - 业务迁移和改造的成本
            + 对业务进行一定的改造和解耦
                * 无状态化
            + 从物理机或虚拟机迁移到容器
                * 容器上线
                * 测试业务功能
                * 知会相关上下游
                * 割接流量
                * 最终下线业务并回收旧机器
        + 镜像的制作和管理成本
            * Dockerfile 的编写和维护
                - 标准化约定一定的规则
    - K8S 的学习使用成本
        + K8S 具有十几个 API，这些 API 的请求参数非常之多，用法较为复杂
+ 传统成本
    + 非线性
    + 硬件负载
    + 动态扩容???
    + 维护成本???
+ 开发测试质量保证运维 vs DevOps
    * 快速迭代 交付
        - 质量 安全 发布周期
    * Infrastructure as Code
        - 可 git 管控
            + 基础设施&部署环境
        - Cloud Native 基础设施在提供自主应用管理的 IaaS 上创建了一个平台, 该平台在动态创建的基础设施之上以抽象出单个服务器并促进动态资源分配调度


---
+ 遥测
    * 多少请求
    * 多久回复, 超时时间
    * 多少错误


---
+ service mesh & 微服务治理
    * Service Mesh 的基础是透明代理，通过 sidecar proxy 拦截到微服务间流量后再通过控制平面配置管理微服务的行为。
        - 应用程序间通讯的中间层
        - 轻量级网络代理
        - 应用程序无感知
    * traffic management, api gateway
        - 将流量管理从 Kubernetes 中解耦
            + 通过为更接近微服务应用层的抽象，管理服务间的流量、安全性和可观察性。
        - vs kub-proxy
            + 对每个服务做细粒度的控制
    * observability
        - 服务调用
        - 性能分析
        - 分布式追踪
    * policy enforcement
        - 控制服务访问策略
            + 多级调用
        - service identity & security
            + 安全保护
    * 标准化的日志
        - 服务化以后，每个服务可以用不同的开发语言
